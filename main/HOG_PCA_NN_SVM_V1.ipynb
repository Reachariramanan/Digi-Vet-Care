{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HOG + PCA + NN + SVM v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Availability :  True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "DEVICES=tf.config.experimental.list_physical_devices()\n",
    "tf.config.experimental.set_memory_growth(device=DEVICES[1],enable=True)\n",
    "print('GPU Availability : ',tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet-50 PCA\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "#import h5py\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "#from pathlib import Path\n",
    "#from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage.feature import hog\n",
    "from sklearn.decomposition import PCA , IncrementalPCA\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam,SGD\n",
    "#from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, Input, InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaimg(image):\n",
    "    blue, green, red = cv2.split(image)\n",
    "    df_red = red/255\n",
    "    df_blue = blue/255\n",
    "    df_green = green/255\n",
    "    pca_b = PCA(n_components=50)\n",
    "    pca_b.fit(df_blue)\n",
    "    trans_pca_b = pca_b.transform(df_blue)\n",
    "    \n",
    "    pca_g = PCA(n_components=50)\n",
    "    pca_g.fit(df_green)\n",
    "    trans_pca_g = pca_g.transform(df_green)\n",
    "    \n",
    "    pca_r = PCA(n_components=50)\n",
    "    pca_r.fit(df_red)\n",
    "    trans_pca_r = pca_r.transform(df_red)    \n",
    "    \n",
    "    b_arr = pca_b.inverse_transform(trans_pca_b)\n",
    "    g_arr = pca_g.inverse_transform(trans_pca_g)\n",
    "    r_arr = pca_r.inverse_transform(trans_pca_r)\n",
    "    \n",
    "    img_reduced = (cv2.merge((b_arr, g_arr, r_arr)))\n",
    "    \n",
    "    return img_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HOG(imagePath,size):\n",
    "    H=[]\n",
    "    image = cv2.imread(imagePath)\n",
    "    imagem = cv2.bitwise_not(image)\n",
    "    imS = cv2.resize(image, (size, size))\n",
    "    gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.3, tileGridSize=(8,8))\n",
    "    img = clahe.apply(gray) \n",
    "    #gray = cv2.cvtColor(imS, cv2.COLOR_BGR2GRAY)\n",
    "    (H, hogImage) = hog(img, orientations=9, pixels_per_cell=(17, 17),cells_per_block=(2,2), transform_sqrt=True, block_norm=\"L1\", visualize=True) \n",
    "    #Data Normalisation (Normalising HOG Features)\n",
    "    H*=10\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256\n",
    "data = []\n",
    "labels = []\n",
    "traindir='./path/Dataset/Vrindhaavan_overall_backup/Vrindavan_overall/'#Enter the Train Directory\n",
    "for directory_path in glob.glob(traindir+\"/*\"):\n",
    "    label = directory_path.split(\"\\\\\")[-1]\n",
    "    print(label)\n",
    "    for img_path in glob.glob(os.path.join(directory_path,\"*.png\")):\n",
    "        #print(img_path)\n",
    "        A = HOG(img_path,SIZE)\n",
    "        #H*=10 #data Normalisation\n",
    "        data.append(A)\n",
    "        labels.append(label)\n",
    "np.save('./HOG_NN/data.npy',data)\n",
    "np.save('./HOG_NN/labels.npy',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16439\n",
      "(16439, 1096)\n"
     ]
    }
   ],
   "source": [
    "data=np.load('./HOG_NN/data.npy')\n",
    "labels=np.load('./HOG_NN/labels.npy')\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "toh= le.transform(labels)\n",
    "OHE_labels=to_categorical(toh)\n",
    "print(len(train_Y))\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16439, 7056)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16439,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(*Counter(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cum variance')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# First verfiy the ideal number of PCA components to not lose much information. \n",
    "# Try to retain 90% information, so look where the curve starts to flatten.\n",
    "# Remember that the n_components must be lower than the number of rows or columns (features)\n",
    "pca_test = PCA(n_components=3000) \n",
    "#pca_test.fit(trainX)\n",
    "pca_test.fit(data)\n",
    "plt.plot(np.cumsum(pca_test.explained_variance_ratio_))\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cum variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick the optimal number of components. This is how many features we will have \n",
    "#for our machine learning\n",
    "\n",
    "n_PCA_components = 1008\n",
    "pca = PCA(n_components=n_PCA_components)\n",
    "train_PCA = pca.fit_transform(data)\n",
    "#test_PCA = pca.transform(data) #Make sure you are just transforming, not fitting.\n",
    "\n",
    "#If we want 90% information captured we can also try ...\n",
    "#pca=PCA(0.9)\n",
    "#train_PCA = pca.fit_transform(data)\n",
    "#n_PCA_components = len(train_PCA)\n",
    "#print(principalComponents.shape)\n",
    "#test_PCA = pca.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "pk.dump(pca, open(\"data.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''n_PCA_components = 1008\n",
    "pca = IncrementalPCA(n_components=n_PCA_components,batch_size=1008)\n",
    "train_PCA = pca.fit_transform(data)\n",
    "test_PCA = pca.transform(data)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(n_PCA_components,)))\n",
    "model.add(Dense(1008,activation='sigmoid'))\n",
    "model.add(Dense(1008,activation='relu'))\n",
    "model.add(Dense(1008,activation='relu'))\n",
    "#model.add(Dense(1008,activation='sigmoid'))\n",
    "model.add(Dense(1096, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./HOG_NN/vrindhavan_model_weights_iteration1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pop()\n",
    "model.add(Dense(1096, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1008)              1017072   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1008)              1017072   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1008)              1017072   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1008)              1017072   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1096)              1105864   \n",
      "=================================================================\n",
      "Total params: 5,174,152\n",
      "Trainable params: 5,174,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16439 samples\n",
      "Epoch 1/108\n",
      "16352/16439 [============================>.] - ETA: 0s - loss: 0.5878 - categorical_accuracy: 0.9329\n",
      "Epoch 00001: categorical_accuracy improved from -inf to 0.93272, saving model to ./HOG_NN/vrindhavan_model_weights_iteration1.h5\n",
      "16439/16439 [==============================] - 12s 730us/sample - loss: 0.5881 - categorical_accuracy: 0.9327\n",
      "Epoch 2/108\n",
      "16384/16439 [============================>.] - ETA: 0s - loss: 0.2514 - categorical_accuracy: 0.9720\n",
      "Epoch 00002: categorical_accuracy improved from 0.93272 to 0.97196, saving model to ./HOG_NN/vrindhavan_model_weights_iteration1.h5\n",
      "16439/16439 [==============================] - 12s 701us/sample - loss: 0.2514 - categorical_accuracy: 0.9720\n",
      "Epoch 3/108\n",
      "16416/16439 [============================>.] - ETA: 0s - loss: 0.2299 - categorical_accuracy: 0.9791\n",
      "Epoch 00003: categorical_accuracy improved from 0.97196 to 0.97907, saving model to ./HOG_NN/vrindhavan_model_weights_iteration1.h5\n",
      "16439/16439 [==============================] - 14s 843us/sample - loss: 0.2317 - categorical_accuracy: 0.9791\n",
      "Epoch 4/108\n",
      "16416/16439 [============================>.] - ETA: 0s - loss: 0.2447 - categorical_accuracy: 0.9792\n",
      "Epoch 00004: categorical_accuracy improved from 0.97907 to 0.97920, saving model to ./HOG_NN/vrindhavan_model_weights_iteration1.h5\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2446 - categorical_accuracy: 0.9792\n",
      "Epoch 5/108\n",
      "16384/16439 [============================>.] - ETA: 0s - loss: 0.2258 - categorical_accuracy: 0.9855\n",
      "Epoch 00005: categorical_accuracy improved from 0.97920 to 0.98552, saving model to ./HOG_NN/vrindhavan_model_weights_iteration1.h5\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2257 - categorical_accuracy: 0.9855\n",
      "Epoch 6/108\n",
      "16384/16439 [============================>.] - ETA: 0s - loss: 0.2671 - categorical_accuracy: 0.9775\n",
      "Epoch 00006: categorical_accuracy did not improve from 0.98552\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2669 - categorical_accuracy: 0.9776\n",
      "Epoch 7/108\n",
      "16416/16439 [============================>.] - ETA: 0s - loss: 0.2816 - categorical_accuracy: 0.9808\n",
      "Epoch 00007: categorical_accuracy did not improve from 0.98552\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2815 - categorical_accuracy: 0.9808\n",
      "Epoch 8/108\n",
      "16416/16439 [============================>.] - ETA: 0s - loss: 0.2815 - categorical_accuracy: 0.9843\n",
      "Epoch 00008: categorical_accuracy did not improve from 0.98552\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2814 - categorical_accuracy: 0.9844\n",
      "Epoch 9/108\n",
      "16384/16439 [============================>.] - ETA: 0s - loss: 0.2439 - categorical_accuracy: 0.9875\n",
      "Epoch 00009: categorical_accuracy improved from 0.98552 to 0.98747, saving model to ./HOG_NN/vrindhavan_model_weights_iteration1.h5\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2443 - categorical_accuracy: 0.9875\n",
      "Epoch 10/108\n",
      "16416/16439 [============================>.] - ETA: 0s - loss: 0.2531 - categorical_accuracy: 0.9837\n",
      "Epoch 00010: categorical_accuracy did not improve from 0.98747\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2530 - categorical_accuracy: 0.9837\n",
      "Epoch 11/108\n",
      "16384/16439 [============================>.] - ETA: 0s - loss: 0.2599 - categorical_accuracy: 0.9850\n",
      "Epoch 00011: categorical_accuracy did not improve from 0.98747\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2599 - categorical_accuracy: 0.9849\n",
      "Epoch 12/108\n",
      "16416/16439 [============================>.] - ETA: 0s - loss: 0.2442 - categorical_accuracy: 0.9865\n",
      "Epoch 00012: categorical_accuracy did not improve from 0.98747\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2441 - categorical_accuracy: 0.9865\n",
      "Epoch 13/108\n",
      "16384/16439 [============================>.] - ETA: 0s - loss: 0.2675 - categorical_accuracy: 0.9833\n",
      "Epoch 00013: categorical_accuracy did not improve from 0.98747\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2679 - categorical_accuracy: 0.9833\n",
      "Epoch 14/108\n",
      "16416/16439 [============================>.] - ETA: 0s - loss: 0.2622 - categorical_accuracy: 0.9838\n",
      "Epoch 00014: categorical_accuracy did not improve from 0.98747\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2622 - categorical_accuracy: 0.9838\n",
      "Epoch 15/108\n",
      "16384/16439 [============================>.] - ETA: 0s - loss: 0.2706 - categorical_accuracy: 0.9843\n",
      "Epoch 00015: categorical_accuracy did not improve from 0.98747\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2707 - categorical_accuracy: 0.9843\n",
      "Epoch 16/108\n",
      "16416/16439 [============================>.] - ETA: 0s - loss: 0.2557 - categorical_accuracy: 0.9861\n",
      "Epoch 00016: categorical_accuracy did not improve from 0.98747\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2556 - categorical_accuracy: 0.9861\n",
      "Epoch 17/108\n",
      "16384/16439 [============================>.] - ETA: 0s - loss: 0.2265 - categorical_accuracy: 0.9872\n",
      "Epoch 00017: categorical_accuracy did not improve from 0.98747\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2270 - categorical_accuracy: 0.9872\n",
      "Epoch 18/108\n",
      "16384/16439 [============================>.] - ETA: 0s - loss: 0.2355 - categorical_accuracy: 0.9869\n",
      "Epoch 00018: categorical_accuracy did not improve from 0.98747\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2359 - categorical_accuracy: 0.9869\n",
      "Epoch 19/108\n",
      "16384/16439 [============================>.] - ETA: 0s - loss: 0.2671 - categorical_accuracy: 0.9805\n",
      "Epoch 00019: categorical_accuracy did not improve from 0.98747\n",
      "16439/16439 [==============================] - 22s 1ms/sample - loss: 0.2669 - categorical_accuracy: 0.9805\n",
      "Epoch 00019: early stopping\n",
      "Total execution time with PCA is:  0:06:31.704052\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "model.load_weights('./HOG_NN/vrindhavan_model_weights_iteration1.h5')\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"./HOG_NN/vrindhavan_model_weights_iteration1.h5\", \n",
    "                             monitor='categorical_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=True, \n",
    "                             mode='auto', \n",
    "                             #save_freq=1\n",
    "                             )\n",
    "\n",
    "early = EarlyStopping(monitor='categorical_accuracy', \n",
    "                      min_delta=0, \n",
    "                      patience=10, \n",
    "                      verbose=1, \n",
    "                      mode='auto')\n",
    "Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.32,\n",
    "    beta_2=0.397,\n",
    "    epsilon=9e-08,\n",
    "    amsgrad=True,\n",
    "    name=\"Adam\", \n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['categorical_accuracy'])\n",
    "\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "#Fit the model. Do not forget to use on-hot-encoded Y values. \n",
    "model.fit(train_PCA,OHE_labels, \n",
    "          epochs=108, \n",
    "          verbose=1,\n",
    "          callbacks=[checkpoint,early]\n",
    "          )\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(\"Total execution time with PCA is: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_PCA = pca.transform(data) #Make sure you are just transforming, not fitting.\n",
    "##Predict on test dataset\n",
    "predict_test = model.predict(test_PCA)\n",
    "predict_test = np.argmax(predict_test, axis=1)\n",
    "predict_test = le.inverse_transform(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.987043007482207\n"
     ]
    }
   ],
   "source": [
    "##Print overall accuracy\n",
    "from sklearn import metrics\n",
    "print (\"Accuracy = \", metrics.accuracy_score(labels, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix - verify accuracy of each class\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#testY=str(testY)\n",
    "cm = confusion_matrix(labels, predict_test)\n",
    "#print(cm)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____0.bead black____\n",
    "____1.bead_flat____\n",
    "____2.bead colored____\n",
    "____3.flat____\n",
    "____4.flat_color____\n",
    "____5.Valley____\n",
    "____6.valley colored____\n",
    "____7.bead valley____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14390\n",
      "(7056,)\n",
      "(1, 7056)\n",
      "[959]\n",
      "The prediction for this image is:  ['6467']\n",
      "The actual label for this image is:  6467\n"
     ]
    }
   ],
   "source": [
    "#Check results on a few select images\n",
    "n=np.random.randint(0,data.shape[0])\n",
    "print(n)\n",
    "val = data[n]\n",
    "print(val.shape)\n",
    "x = np.expand_dims(val, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
    "print(x.shape)\n",
    "# #Expand dims so the input is (num images, x, y, c)\n",
    "input_img_feature=x\n",
    "input_img_PCA = pca.transform(input_img_feature)\n",
    "prediction_img = model.predict(input_img_PCA)\n",
    "prediction_img = np.argmax(prediction_img, axis=1)\n",
    "print(prediction_img)\n",
    "prediction_img = le.inverse_transform(prediction_img)  #Reverse the label encoder to original name\n",
    "print(\"The prediction for this image is: \", prediction_img)\n",
    "print(\"The actual label for this image is: \", labels[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Vrindhavan_Overall_sukirti.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE=256\n",
    "imagepath='./path/Dataset/Vrindhaavan_overall_backup/Vrindavan_overall/1464/Chatshikhar_Cow_Female_13.png'\n",
    "H=HOG(imagepath,SIZE)\n",
    "x = np.expand_dims(H, axis=0)\n",
    "input_img_feature=x\n",
    "input_img_PCA = pca.transform(input_img_feature)\n",
    "prediction_img = model.predict(input_img_PCA)\n",
    "prediction_img = np.argmax(prediction_img, axis=1)\n",
    "print(prediction_img)\n",
    "prediction_img = le.inverse_transform(prediction_img)  #Reverse the label encoder to original name\n",
    "print(\"The prediction for this image is: \", prediction_img)\n",
    "#print(\"The actual label for this image is: \", test_labels[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "SIZE=256\n",
    "imagepath='./path/Dataset/Vrindhaavan_overall_backup/Vrindavan_overall/1464/Chatshikhar_Cow_Female_13.png'\n",
    "H=HOG(imagepath,SIZE)\n",
    "x = np.expand_dims(H, axis=0)\n",
    "input_img_feature=x\n",
    "input_img_PCA = pca.fit_transform(input_img_feature)\n",
    "all_model=load_('./Vrindhavan_Overall_sukirti.h5')\n",
    "prediction_img = all_model.predict(input_img_PCA)\n",
    "prediction_img = np.argmax(prediction_img, axis=1)\n",
    "print(prediction_img)\n",
    "prediction_img = le.inverse_transform(prediction_img)  #Reverse the label encoder to original name\n",
    "print(\"The prediction for this image is: \", prediction_img)\n",
    "#print(\"The actual label for this image is: \", test_labels[n])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('GPU_ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81d8000a49e387cd3bf236de4b1b0050e70533eb5acd278118fdc7c0243f6cc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
